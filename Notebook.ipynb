{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file2=pd.read_csv(\"Sentiment_Analysis Dataset.csv\",error_bad_lines=False)\n",
    "\n",
    "file2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "Text=list(file2.SentimentText)\n",
    "file2.groupby('Sentiment').count()\n",
    "train_data,test_data,train_label,test_label=train_test_split(Text, file2.Sentiment, test_size=0.4, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocabDic=dict()\n",
    "for document in train_data:\n",
    "    document=document.split(\" \")\n",
    "    for word in document:\n",
    "        if word not in vocabDic:\n",
    "            vocabDic[word]=len(vocabDic)+1\n",
    "vocabDic['unk']=len(vocabDic)+1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def codeDocuments(documents,dictionnary):\n",
    "    documentsArray=list()\n",
    "    for i,document in enumerate(documents):\n",
    "        tempList=list()\n",
    "        document=document.split(\" \")\n",
    "        for word in document:\n",
    "            if word in vocabDic:\n",
    "                word=vocabDic[word]\n",
    "            else:\n",
    "                word=vocabDic['unk']\n",
    "            tempList.append(word)\n",
    "        documentsArray.append(tempList)\n",
    "    return np.array(documentsArray)\n",
    "train_docs=codeDocuments(train_data,vocabDic)\n",
    "test_docs=codeDocuments(test_data,vocabDic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean(numbers):\n",
    "    return float(sum(numbers)) / max(len(numbers), 1)\n",
    "\n",
    "avgList=mean([len(x) for x in train_docs])\n",
    "print(avgList)\n",
    "max([len(x) for x in train_docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "maxlen=75\n",
    "train_set = sequence.pad_sequences(train_docs, maxlen=maxlen)\n",
    "test_set = sequence.pad_sequences(test_docs, maxlen=maxlen)\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "#test_labels_encoded=mlb.fit_transform(train_label)\n",
    "#train_labels_encoded=mlb.fit_transform(test_label)\n",
    "type(train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label=np.array(train_label)\n",
    "test_label=np.array(test_label)\n",
    "len(train_label)\n",
    "len(test_label)\n",
    "train_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Kim's CNN : https://arxiv.org/pdf/1408.5882.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv2D\n",
    "from keras.models import Sequential\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Reshape\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Input\n",
    "from keras import backend as K\n",
    "\n",
    "from keras.models import Model\n",
    "import tensorflow as tf\n",
    "from sklearn import preprocessing\n",
    "import keras\n",
    "import numpy as np\n",
    "import pdb\n",
    "\n",
    "#using multiple filters\n",
    "config = tf.ConfigProto()\n",
    " \n",
    "# Don't pre-allocate memory; allocate as-needed\n",
    "config.gpu_options.allow_growth = True\n",
    " \n",
    "# Only allow a total of half the GPU memory to be allocated\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.7\n",
    " \n",
    "# Create a session with the above options specified.\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.65\n",
    "K.tensorflow_backend.set_session(tf.Session(config=config))\n",
    "class Classifier():\n",
    "    def __init__(self,Vocab,maxlen=75):\n",
    "        self.model=Sequential()\n",
    "        self.EMBED_DIM=30\n",
    "        self.batch_size=30\n",
    "        self.MAX_SEQUENCE_LENGTH=maxlen\n",
    "        self.nb_labels=2\n",
    "        self.model = Sequential()\n",
    "        self.Vocab=Vocab\n",
    "    def fit(self, X,y):\n",
    "        #pdb.set_trace()\n",
    "        \n",
    "        mainIn=Input(shape=(self.MAX_SEQUENCE_LENGTH,), dtype='int32', name='main_input')\n",
    "        \n",
    "        x=Embedding(len(self.Vocab)+2,self.EMBED_DIM,input_length=self.MAX_SEQUENCE_LENGTH)(mainIn)\n",
    "        \n",
    "        x=Reshape((1,self.MAX_SEQUENCE_LENGTH, self.EMBED_DIM))(x)\n",
    "        \n",
    "        x1=Conv2D(128, strides=2,kernel_size=5 ,activation=\"relu\", padding='same')(x)\n",
    "\n",
    "        x1=MaxPooling2D((self.MAX_SEQUENCE_LENGTH-5+1,1),padding='same')(x1)\n",
    "        x1=Flatten()(x1)\n",
    "        x2=Conv2D(128, strides=2, kernel_size=4, activation=\"sigmoid\", padding='same')(x)\n",
    "        x2=MaxPooling2D((self.MAX_SEQUENCE_LENGTH-4+1,1),padding='same')(x2)\n",
    "        \n",
    "        x2=Flatten()(x2)\n",
    "        x3=Conv2D(128, strides=2, kernel_size=3, activation=\"tanh\", padding='same')(x)\n",
    "        x3=MaxPooling2D((self.MAX_SEQUENCE_LENGTH-3+1,1),padding='same')(x3)\n",
    "        \n",
    "        x3=Flatten()(x3)\n",
    "        \n",
    "        combinedX=keras.layers.concatenate([x1,x2,x3],axis=1)\n",
    "        \n",
    "        \n",
    "        combinedX=Dense(128, activation=\"relu\")(combinedX)\n",
    "        combinedX=Dropout(0.2)(combinedX)\n",
    "        #output=Dense(self.nb_labels, activation=\"sigmoid\")(combinedX)\n",
    "        output=Dense(2, activation=\"softmax\")(combinedX)\n",
    "        #output=Dense(1, activation=\"sigmoid\")(combinedX)\n",
    "\n",
    "        \n",
    "        encoder =preprocessing.LabelEncoder()\n",
    "        encoder.fit(y)\n",
    "        encoded_Y = encoder.transform(y)\n",
    "        labels=keras.utils.to_categorical(encoded_Y, num_classes=2)\n",
    "        #labels=y\n",
    "        inputs2=X\n",
    "        self.model = Model(inputs=mainIn, outputs=output)\n",
    "        \n",
    "       # self.model.compile(loss='binary_crossentropy',\n",
    "       #                 optimizer='adam',\n",
    "       #                 metrics=['acc'])\n",
    "        \n",
    "        self.model.compile(loss='categorical_crossentropy',\n",
    "                        optimizer='adam',\n",
    "                        metrics=['acc'])\n",
    "        \n",
    "        self.model.fit(inputs2,labels,epochs=7, batch_size=self.batch_size)\n",
    "    def predict(self, X):\n",
    "        return self.model.predict(np.array(X))\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \n",
    "        return self.model.predict(np.array(X), self.batch_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model=Classifier(vocabDic,maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model=Classifier(vocabDic,maxlen)\n",
    "model.fit(train_set[:50000],train_label[:50000])\n",
    "\n",
    "\n",
    "#model.fit(train_set[:50],train_label[:50])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions=model.predict(test_set)\n",
    "predicted_labels=np.argmax(predictions,axis=1)\n",
    "correct_predictions=sum(predicted_labels==test_label)\n",
    "print(correct_predictions/len(test_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a simple LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv2D\n",
    "from keras.models import Sequential\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Reshape\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Input\n",
    "from keras import backend as K\n",
    "from keras.layers import LSTM\n",
    "from keras.models import Model\n",
    "import tensorflow as tf\n",
    "from sklearn import preprocessing\n",
    "import keras\n",
    "import numpy as np\n",
    "import pdb\n",
    "\n",
    "#using multiple filters\n",
    "config = tf.ConfigProto()\n",
    " \n",
    "# Don't pre-allocate memory; allocate as-needed\n",
    "config.gpu_options.allow_growth = True\n",
    " \n",
    "# Only allow a total of half the GPU memory to be allocated\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
    " \n",
    "# Create a session with the above options specified.\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
    "K.tensorflow_backend.set_session(tf.Session(config=config))\n",
    "class ClassifierLSTM():\n",
    "    def __init__(self,Vocab,maxlen):\n",
    "        self.model=Sequential()\n",
    "        self.EMBED_DIM=30\n",
    "        self.batch_size=30\n",
    "        self.MAX_SEQUENCE_LENGTH=maxlen\n",
    "        self.nb_labels=2\n",
    "        self.model = Sequential()\n",
    "        self.Vocab=Vocab\n",
    "    def useWeights(self,embed_dim,X):\n",
    "        embeddings_index = {}\n",
    "        VOCAB=X['statements'][1].keys()\n",
    "        vocabDic=VOCAB=X['statements'][1]\n",
    "        EMBEDDING_DIM=embed_dim\n",
    "        embedding_matrix = np.zeros((len(VOCAB) + 1, EMBEDDING_DIM))\n",
    "        i=0\n",
    "        with open('wiki-news-300d-1M.vec') as f:\n",
    "            next(f)\n",
    "            for line in f:\n",
    "                values = line.split()\n",
    "                word = values[0]\n",
    "\n",
    "                if word in VOCAB:\n",
    "                    embedding_matrix[vocabDic[word],:]=np.asarray(values[1:EMBEDDING_DIM+1], dtype='float32')\n",
    "                    i=i+1\n",
    "        return (embedding_matrix)\n",
    "    \n",
    "    def fit(self, X,y):\n",
    "        #pdb.set_trace()\n",
    "        \n",
    "        mainIn=Input(shape=(self.MAX_SEQUENCE_LENGTH,), dtype='int32', name='main_input')\n",
    "        \n",
    "        x=Embedding(len(self.Vocab)+2,self.EMBED_DIM,input_length=self.MAX_SEQUENCE_LENGTH)(mainIn)\n",
    "        \n",
    "        #x=Reshape((1,self.MAX_SEQUENCE_LENGTH, self.EMBED_DIM))(x)\n",
    "        \n",
    "        x=LSTM(self.EMBED_DIM, dropout=0.2, recurrent_dropout=0.2)(x)\n",
    "        output=Dense(1, activation='sigmoid')(x)\n",
    "        \n",
    "        #encoder =preprocessing.LabelEncoder()\n",
    "        #encoder.fit(y)\n",
    "        #encoded_Y = encoder.transform(y)\n",
    "        #labels=keras.utils.to_categorical(encoded_Y, num_classes=2)\n",
    "        labels=y\n",
    "        inputs=X\n",
    "        self.model = Model(inputs=mainIn, outputs=output)\n",
    "        \n",
    "        self.model.compile(loss='binary_crossentropy',\n",
    "                        optimizer='adam',\n",
    "                        metrics=['acc'])\n",
    "        self.model.summary\n",
    "        self.model.fit(inputs,labels,epochs=3, batch_size=self.batch_size)\n",
    "    def predict(self, X):\n",
    "        return self.model.predict(np.array(X))\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \n",
    "        return self.model.predict(np.array(X), self.batch_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model=ClassifierLSTM(vocabDic,maxlen)\n",
    "model.fit(train_set[:500000],train_label[:500000])\n",
    "predictions=model.predict(test_set)\n",
    "predicted_labels=np.argmax(predictions,axis=1)\n",
    "correct_predictions=sum(predicted_labels==test_label)\n",
    "print(correct_predictions/len(test_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions_bool=predictions>0.5\n",
    "predictions_bool=[1 if x ==True else 0 for x in predictions_bool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "predicted_labels=predictions>0.5\n",
    "correct_predictions=sum(predictions_bool==test_label)\n",
    "print(correct_predictions/len(test_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv2D\n",
    "from keras.models import Sequential\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Reshape\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Input\n",
    "from keras.layers import Bidirectional\n",
    "from keras import backend as K\n",
    "from keras.layers import LSTM\n",
    "from keras.models import Model\n",
    "import tensorflow as tf\n",
    "from sklearn import preprocessing\n",
    "import keras\n",
    "import numpy as np\n",
    "import pdb\n",
    "\n",
    "#using multiple filters\n",
    "config = tf.ConfigProto()\n",
    " \n",
    "# Don't pre-allocate memory; allocate as-needed\n",
    "config.gpu_options.allow_growth = True\n",
    " \n",
    "# Only allow a total of half the GPU memory to be allocated\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
    " \n",
    "# Create a session with the above options specified.\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
    "K.tensorflow_backend.set_session(tf.Session(config=config))\n",
    "class ClassifierBidirectionalLSTM():\n",
    "    def __init__(self,Vocab,maxlen):\n",
    "        self.model=Sequential()\n",
    "        self.EMBED_DIM=30\n",
    "        self.batch_size=30\n",
    "        self.MAX_SEQUENCE_LENGTH=maxlen\n",
    "        self.nb_labels=2\n",
    "        self.model = Sequential()\n",
    "        self.Vocab=Vocab\n",
    "    def useWeights(self,embed_dim,X):\n",
    "        embeddings_index = {}\n",
    "        VOCAB=X['statements'][1].keys()\n",
    "        vocabDic=VOCAB=X['statements'][1]\n",
    "        EMBEDDING_DIM=embed_dim\n",
    "        embedding_matrix = np.zeros((len(VOCAB) + 1, EMBEDDING_DIM))\n",
    "        i=0\n",
    "        with open('wiki-news-300d-1M.vec') as f:\n",
    "            next(f)\n",
    "            for line in f:\n",
    "                values = line.split()\n",
    "                word = values[0]\n",
    "\n",
    "                if word in VOCAB:\n",
    "                    embedding_matrix[vocabDic[word],:]=np.asarray(values[1:EMBEDDING_DIM+1], dtype='float32')\n",
    "                    i=i+1\n",
    "        return (embedding_matrix)\n",
    "    \n",
    "    def fit(self, X,y):\n",
    "        #pdb.set_trace()\n",
    "        \n",
    "        mainIn=Input(shape=(self.MAX_SEQUENCE_LENGTH,), dtype='int32', name='main_input')\n",
    "        \n",
    "        x=Embedding(len(self.Vocab)+2,self.EMBED_DIM,input_length=self.MAX_SEQUENCE_LENGTH)(mainIn)\n",
    "        \n",
    "        #x=Reshape((1,self.MAX_SEQUENCE_LENGTH, self.EMBED_DIM))(x)\n",
    "        \n",
    "        x=Bidirectional(LSTM(self.EMBED_DIM, dropout=0.2, recurrent_dropout=0.2))(x)\n",
    "        output=Dense(1, activation='sigmoid')(x)\n",
    "        \n",
    "        #encoder =preprocessing.LabelEncoder()\n",
    "        #encoder.fit(y)\n",
    "        #encoded_Y = encoder.transform(y)\n",
    "        #labels=keras.utils.to_categorical(encoded_Y, num_classes=2)\n",
    "        labels=y\n",
    "        inputs=X\n",
    "        self.model = Model(inputs=mainIn, outputs=output)\n",
    "        \n",
    "        self.model.compile(loss='binary_crossentropy',\n",
    "                        optimizer='adam',\n",
    "                        metrics=['acc'])\n",
    "        self.model.summary\n",
    "        self.model.fit(inputs,labels,epochs=7, batch_size=self.batch_size)\n",
    "    def predict(self, X):\n",
    "        return self.model.predict(np.array(X))\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \n",
    "        return self.model.predict(np.array(X), self.batch_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model=ClassifierBidirectionalLSTM(vocabDic,maxlen)\n",
    "model.fit(train_set[:500000],train_label[:500000])\n",
    "predictions=model.predict(test_set)\n",
    "predictions_bool=predictions>0.5\n",
    "predictions_bool=[1 if x ==True else 0 for x in predictions_bool]\n",
    "correct_predictions=sum(predictions_bool==test_label)\n",
    "print(correct_predictions/len(test_label))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
