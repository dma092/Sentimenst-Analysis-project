{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipping line 8836: expected 4 fields, saw 5\n",
      "\n",
      "Skipping line 535882: expected 4 fields, saw 7\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ItemID</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentSource</th>\n",
       "      <th>SentimentText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>is so sad for my APL frie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>I missed the New Moon trail...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>omg its already 7:30 :O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>.. Omgaga. Im sooo  im gunna CRy. I'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>i think mi bf is cheating on me!!!   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>or i just worry too much?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>Juuuuuuuuuuuuuuuuussssst Chillin!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>Sunny Again        Work Tomorrow  :-|  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>handed in my uniform today . i miss you ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>hmmmm.... i wonder how she my number @-)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>I must think about positive..</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>thanks to all the haters up in my face a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>this weekend has sucked so far</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>jb isnt showing in australia any more!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>ok thats it you win.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>&amp;lt;-------- This is the way i feel right ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>awhhe man.... I'm completely useless rt no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>Feeling strangely fine. Now I'm gonna go l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>HUGE roll of thunder just now...SO scary!!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>I just cut my beard off. It's only been gr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>Very sad about Iran.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>wompppp wompp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>You're the only one who can see this cause...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>&amp;lt;---Sad level is 3. I was writing a mass...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>...  Headed to Hospitol : Had to pull out o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>BoRinG   ): whats wrong with him??     Plea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>can't be bothered. i wish i could spend the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>Feeeling like shit right now. I really want...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>goodbye exams, HELLO ALCOHOL TONIGHT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>I didn't realize it was THAT deep. Geez giv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1578582</th>\n",
       "      <td>1578598</td>\n",
       "      <td>1</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>zoo was rad today. feeling tired and not motiv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1578583</th>\n",
       "      <td>1578599</td>\n",
       "      <td>1</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>Zoo with the woman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1578584</th>\n",
       "      <td>1578600</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>zoolander and alice in wonderland. i have a ki...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1578585</th>\n",
       "      <td>1578601</td>\n",
       "      <td>1</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>Zoom zoom! Back to bristol today I have my bea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1578586</th>\n",
       "      <td>1578602</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>zootm: cannot survive without CRLF support  - ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1578587</th>\n",
       "      <td>1578603</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>Zoran lost Croatian Idol!  The difference was ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1578588</th>\n",
       "      <td>1578604</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>Zork. Buggy beta version</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1578589</th>\n",
       "      <td>1578605</td>\n",
       "      <td>1</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>Zow, finished uploading pictures on Flickr and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1578590</th>\n",
       "      <td>1578606</td>\n",
       "      <td>1</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>Zrock was awesome!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1578591</th>\n",
       "      <td>1578607</td>\n",
       "      <td>1</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>ZTecWiz bought mIRC for $10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1578592</th>\n",
       "      <td>1578608</td>\n",
       "      <td>1</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>'Zu SpÃ¤t' by Die Ã„rzte. One of the best band...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1578593</th>\n",
       "      <td>1578609</td>\n",
       "      <td>1</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>Zuma bitch tomorrow. Have a wonderful night ev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1578594</th>\n",
       "      <td>1578610</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>zummie's couch tour was amazing....to bad i ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1578595</th>\n",
       "      <td>1578611</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>ZuneHD looks great! OLED screen @720p, HDMI, o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1578596</th>\n",
       "      <td>1578612</td>\n",
       "      <td>1</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>zup there ! learning a new magic trick</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1578597</th>\n",
       "      <td>1578613</td>\n",
       "      <td>1</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>zyklonic showers   *evil*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1578598</th>\n",
       "      <td>1578614</td>\n",
       "      <td>1</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>ZZ Top â€“ I Thank You ...@hawaiibuzz   .....T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1578599</th>\n",
       "      <td>1578615</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>zzz time. Just wish my love could B nxt 2 me</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1578600</th>\n",
       "      <td>1578616</td>\n",
       "      <td>1</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>zzz twitter. good day today. got a lot accompl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1578601</th>\n",
       "      <td>1578617</td>\n",
       "      <td>1</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>zzz's time, goodnight.  http://plurk.com/p/ri9qn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1578602</th>\n",
       "      <td>1578618</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>Zzzz lying in bed watching the countryside thr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1578603</th>\n",
       "      <td>1578619</td>\n",
       "      <td>1</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>Zzzz... Fuck Ã¼ : Zzzz... Fuck Ã¼  http://bit....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1578604</th>\n",
       "      <td>1578620</td>\n",
       "      <td>1</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>Zzzz...no work tomorrow..yayyy!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1578605</th>\n",
       "      <td>1578621</td>\n",
       "      <td>1</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>ZZZZZ time.. Tomorrow will be a busy day for s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1578606</th>\n",
       "      <td>1578622</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>Zzzzz want to sleep but at sister's in-laws's ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1578607</th>\n",
       "      <td>1578623</td>\n",
       "      <td>1</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>Zzzzzz.... Finally! Night tweeters!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1578608</th>\n",
       "      <td>1578624</td>\n",
       "      <td>1</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>Zzzzzzz, sleep well people</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1578609</th>\n",
       "      <td>1578625</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>ZzzZzZzzzZ... wait no I have homework.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1578610</th>\n",
       "      <td>1578626</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>ZzZzzzZZZZzzz meh, what am I doing up again?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1578611</th>\n",
       "      <td>1578627</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>Zzzzzzzzzzzzzzzzzzz, I wish</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1578612 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          ItemID  Sentiment SentimentSource  \\\n",
       "0              1          0    Sentiment140   \n",
       "1              2          0    Sentiment140   \n",
       "2              3          1    Sentiment140   \n",
       "3              4          0    Sentiment140   \n",
       "4              5          0    Sentiment140   \n",
       "5              6          0    Sentiment140   \n",
       "6              7          1    Sentiment140   \n",
       "7              8          0    Sentiment140   \n",
       "8              9          1    Sentiment140   \n",
       "9             10          1    Sentiment140   \n",
       "10            11          0    Sentiment140   \n",
       "11            12          1    Sentiment140   \n",
       "12            13          0    Sentiment140   \n",
       "13            14          0    Sentiment140   \n",
       "14            15          0    Sentiment140   \n",
       "15            16          0    Sentiment140   \n",
       "16            17          0    Sentiment140   \n",
       "17            18          1    Sentiment140   \n",
       "18            19          0    Sentiment140   \n",
       "19            20          0    Sentiment140   \n",
       "20            21          0    Sentiment140   \n",
       "21            22          0    Sentiment140   \n",
       "22            23          1    Sentiment140   \n",
       "23            24          0    Sentiment140   \n",
       "24            25          0    Sentiment140   \n",
       "25            26          0    Sentiment140   \n",
       "26            27          0    Sentiment140   \n",
       "27            28          0    Sentiment140   \n",
       "28            29          1    Sentiment140   \n",
       "29            30          0    Sentiment140   \n",
       "...          ...        ...             ...   \n",
       "1578582  1578598          1    Sentiment140   \n",
       "1578583  1578599          1    Sentiment140   \n",
       "1578584  1578600          0    Sentiment140   \n",
       "1578585  1578601          1    Sentiment140   \n",
       "1578586  1578602          0    Sentiment140   \n",
       "1578587  1578603          0    Sentiment140   \n",
       "1578588  1578604          0    Sentiment140   \n",
       "1578589  1578605          1    Sentiment140   \n",
       "1578590  1578606          1    Sentiment140   \n",
       "1578591  1578607          1    Sentiment140   \n",
       "1578592  1578608          1    Sentiment140   \n",
       "1578593  1578609          1    Sentiment140   \n",
       "1578594  1578610          0    Sentiment140   \n",
       "1578595  1578611          0    Sentiment140   \n",
       "1578596  1578612          1    Sentiment140   \n",
       "1578597  1578613          1    Sentiment140   \n",
       "1578598  1578614          1    Sentiment140   \n",
       "1578599  1578615          0    Sentiment140   \n",
       "1578600  1578616          1    Sentiment140   \n",
       "1578601  1578617          1    Sentiment140   \n",
       "1578602  1578618          0    Sentiment140   \n",
       "1578603  1578619          1    Sentiment140   \n",
       "1578604  1578620          1    Sentiment140   \n",
       "1578605  1578621          1    Sentiment140   \n",
       "1578606  1578622          0    Sentiment140   \n",
       "1578607  1578623          1    Sentiment140   \n",
       "1578608  1578624          1    Sentiment140   \n",
       "1578609  1578625          0    Sentiment140   \n",
       "1578610  1578626          0    Sentiment140   \n",
       "1578611  1578627          0    Sentiment140   \n",
       "\n",
       "                                             SentimentText  \n",
       "0                             is so sad for my APL frie...  \n",
       "1                           I missed the New Moon trail...  \n",
       "2                                  omg its already 7:30 :O  \n",
       "3                  .. Omgaga. Im sooo  im gunna CRy. I'...  \n",
       "4                 i think mi bf is cheating on me!!!   ...  \n",
       "5                        or i just worry too much?          \n",
       "6                       Juuuuuuuuuuuuuuuuussssst Chillin!!  \n",
       "7               Sunny Again        Work Tomorrow  :-|  ...  \n",
       "8              handed in my uniform today . i miss you ...  \n",
       "9                 hmmmm.... i wonder how she my number @-)  \n",
       "10                           I must think about positive..  \n",
       "11             thanks to all the haters up in my face a...  \n",
       "12                          this weekend has sucked so far  \n",
       "13                  jb isnt showing in australia any more!  \n",
       "14                                    ok thats it you win.  \n",
       "15           &lt;-------- This is the way i feel right ...  \n",
       "16           awhhe man.... I'm completely useless rt no...  \n",
       "17           Feeling strangely fine. Now I'm gonna go l...  \n",
       "18            HUGE roll of thunder just now...SO scary!!!!  \n",
       "19           I just cut my beard off. It's only been gr...  \n",
       "20                                    Very sad about Iran.  \n",
       "21                                           wompppp wompp  \n",
       "22           You're the only one who can see this cause...  \n",
       "23          &lt;---Sad level is 3. I was writing a mass...  \n",
       "24          ...  Headed to Hospitol : Had to pull out o...  \n",
       "25          BoRinG   ): whats wrong with him??     Plea...  \n",
       "26          can't be bothered. i wish i could spend the...  \n",
       "27          Feeeling like shit right now. I really want...  \n",
       "28                   goodbye exams, HELLO ALCOHOL TONIGHT   \n",
       "29          I didn't realize it was THAT deep. Geez giv...  \n",
       "...                                                    ...  \n",
       "1578582  zoo was rad today. feeling tired and not motiv...  \n",
       "1578583                                Zoo with the woman   \n",
       "1578584  zoolander and alice in wonderland. i have a ki...  \n",
       "1578585  Zoom zoom! Back to bristol today I have my bea...  \n",
       "1578586  zootm: cannot survive without CRLF support  - ...  \n",
       "1578587  Zoran lost Croatian Idol!  The difference was ...  \n",
       "1578588                          Zork. Buggy beta version   \n",
       "1578589  Zow, finished uploading pictures on Flickr and...  \n",
       "1578590                               Zrock was awesome!!   \n",
       "1578591                       ZTecWiz bought mIRC for $10   \n",
       "1578592  'Zu SpÃ¤t' by Die Ã„rzte. One of the best band...  \n",
       "1578593  Zuma bitch tomorrow. Have a wonderful night ev...  \n",
       "1578594  zummie's couch tour was amazing....to bad i ha...  \n",
       "1578595  ZuneHD looks great! OLED screen @720p, HDMI, o...  \n",
       "1578596            zup there ! learning a new magic trick   \n",
       "1578597                          zyklonic showers   *evil*  \n",
       "1578598  ZZ Top â€“ I Thank You ...@hawaiibuzz   .....T...  \n",
       "1578599      zzz time. Just wish my love could B nxt 2 me   \n",
       "1578600  zzz twitter. good day today. got a lot accompl...  \n",
       "1578601   zzz's time, goodnight.  http://plurk.com/p/ri9qn  \n",
       "1578602  Zzzz lying in bed watching the countryside thr...  \n",
       "1578603  Zzzz... Fuck Ã¼ : Zzzz... Fuck Ã¼  http://bit....  \n",
       "1578604                  Zzzz...no work tomorrow..yayyy!!   \n",
       "1578605  ZZZZZ time.. Tomorrow will be a busy day for s...  \n",
       "1578606  Zzzzz want to sleep but at sister's in-laws's ...  \n",
       "1578607               Zzzzzz.... Finally! Night tweeters!   \n",
       "1578608                        Zzzzzzz, sleep well people   \n",
       "1578609            ZzzZzZzzzZ... wait no I have homework.   \n",
       "1578610      ZzZzzzZZZZzzz meh, what am I doing up again?   \n",
       "1578611                       Zzzzzzzzzzzzzzzzzzz, I wish   \n",
       "\n",
       "[1578612 rows x 4 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file2=pd.read_csv(\"Sentiment Analysis Dataset.csv\",error_bad_lines=False)\n",
    "\n",
    "file2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "Text=list(file2.SentimentText)\n",
    "file2.groupby('Sentiment').count()\n",
    "train_data,test_data,train_label,test_label=train_test_split(Text, file2.Sentiment, test_size=0.4, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabDic=dict()\n",
    "for document in train_data:\n",
    "    document=document.split(\" \")\n",
    "    for word in document:\n",
    "        if word not in vocabDic:\n",
    "            vocabDic[word]=len(vocabDic)+1\n",
    "vocabDic['unk']=len(vocabDic)+1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1198205    0\n",
       "952994     1\n",
       "177883     0\n",
       "234216     0\n",
       "1129158    0\n",
       "553408     1\n",
       "939580     1\n",
       "1183897    0\n",
       "252965     1\n",
       "62810      0\n",
       "186728     1\n",
       "1347935    1\n",
       "742691     0\n",
       "188754     1\n",
       "1031059    0\n",
       "1109469    0\n",
       "288402     0\n",
       "840142     1\n",
       "183699     0\n",
       "1306814    1\n",
       "919709     0\n",
       "345093     0\n",
       "560454     0\n",
       "162356     0\n",
       "1496861    1\n",
       "651107     0\n",
       "1735       1\n",
       "734344     1\n",
       "1129946    0\n",
       "1322275    1\n",
       "          ..\n",
       "1470485    1\n",
       "1396025    1\n",
       "184779     0\n",
       "1262752    0\n",
       "1284372    0\n",
       "103355     0\n",
       "791743     0\n",
       "1247617    0\n",
       "327069     1\n",
       "1370455    0\n",
       "787201     1\n",
       "1113396    1\n",
       "329365     1\n",
       "41090      1\n",
       "278167     1\n",
       "1239911    1\n",
       "175203     0\n",
       "912756     1\n",
       "1136074    1\n",
       "1570006    1\n",
       "999890     0\n",
       "137337     0\n",
       "1103462    0\n",
       "732180     0\n",
       "110268     1\n",
       "259178     1\n",
       "1414414    0\n",
       "131932     1\n",
       "671155     0\n",
       "121958     0\n",
       "Name: Sentiment, Length: 947167, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def codeDocuments(documents,dictionnary):\n",
    "    documentsArray=list()\n",
    "    for i,document in enumerate(documents):\n",
    "        tempList=list()\n",
    "        document=document.split(\" \")\n",
    "        for word in document:\n",
    "            if word in vocabDic:\n",
    "                word=vocabDic[word]\n",
    "            else:\n",
    "                word=vocabDic['unk']\n",
    "            tempList.append(word)\n",
    "        documentsArray.append(tempList)\n",
    "    return np.array(documentsArray)\n",
    "train_docs=codeDocuments(train_data,vocabDic)\n",
    "test_docs=codeDocuments(test_data,vocabDic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list([1, 2, 3, 4, 5, 6, 7, 8, 9]),\n",
       "       list([10, 11, 12, 13, 14, 5, 15]),\n",
       "       list([16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 19, 27, 28, 29, 12, 30, 31, 32, 33, 34, 35, 36, 5]),\n",
       "       ...,\n",
       "       list([933373, 382, 221, 15987, 933374, 2210, 644946, 464, 38627, 5, 5, 53, 403, 2, 86, 17728, 41, 301, 1234, 2436, 8, 195824, 7579, 5]),\n",
       "       list([136133, 114, 466, 40, 64, 933375, 5]),\n",
       "       list([933376, 41, 662, 1287, 549, 22, 5])], dtype=object)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.476622390771638\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "101"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mean(numbers):\n",
    "    return float(sum(numbers)) / max(len(numbers), 1)\n",
    "\n",
    "avgList=mean([len(x) for x in train_docs])\n",
    "print(avgList)\n",
    "max([len(x) for x in train_docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "maxlen=25\n",
    "train_set = sequence.pad_sequences(train_docs, maxlen=maxlen)\n",
    "test_set = sequence.pad_sequences(test_docs, maxlen=maxlen)\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "#test_labels_encoded=mlb.fit_transform(train_label)\n",
    "#train_labels_encoded=mlb.fit_transform(test_label)\n",
    "type(train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[     0,      0,      0, ...,      7,      8,      9],\n",
       "       [     0,      0,      0, ...,     14,      5,     15],\n",
       "       [     0,     16,     17, ...,     35,     36,      5],\n",
       "       ..., \n",
       "       [     0, 933373,    382, ..., 195824,   7579,      5],\n",
       "       [     0,      0,      0, ...,     64, 933375,      5],\n",
       "       [     0,      0,      0, ...,    549,     22,      5]], dtype=int32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, ..., 1, 0, 0])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label=np.array(train_label)\n",
    "test_label=np.array(test_label)\n",
    "len(train_label)\n",
    "len(test_label)\n",
    "train_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Kim's CNN : https://arxiv.org/pdf/1408.5882.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv2D\n",
    "from keras.models import Sequential\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Reshape\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Input\n",
    "from keras import backend as K\n",
    "\n",
    "from keras.models import Model\n",
    "import tensorflow as tf\n",
    "from sklearn import preprocessing\n",
    "import keras\n",
    "import numpy as np\n",
    "import pdb\n",
    "\n",
    "\n",
    "#So far, this is shit\n",
    "\n",
    "\n",
    "\n",
    "#using multiple filters and adding the headline data\n",
    "config = tf.ConfigProto()\n",
    " \n",
    "# Don't pre-allocate memory; allocate as-needed\n",
    "config.gpu_options.allow_growth = True\n",
    " \n",
    "# Only allow a total of half the GPU memory to be allocated\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
    " \n",
    "# Create a session with the above options specified.\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
    "K.tensorflow_backend.set_session(tf.Session(config=config))\n",
    "class Classifier():\n",
    "    def __init__(self,vocabDic,max_len=25,nb_labels=3):\n",
    "    #X=np.array([newEditors,newJobs,newReasearchers,newSources,newStates,newSentences,len(editorsDic),len(jobsDic),\\\n",
    "     # len(reasearchersDic),len(sourcesDic),len(statesDic),len(vocabDic)])\n",
    "        self.EMBED_DIM=30\n",
    "        self.batch_size=30\n",
    "        self.MAX_SEQUENCE_LENGTH=max_len\n",
    "        self.nb_labels=nb_labels\n",
    "        self.vocabDic=vocabDic\n",
    "        self.model = Sequential()\n",
    "        #X=np.array([newEditors,newJobs,newReasearchers,newSources,newStates,newSubjects,newSentences,len(editorsDic),\n",
    "        #len(jobsDic),len(reasearchersDic),len(sourcesDic),len(statesDic),len(subjectsDic),len(vocabDic)])\n",
    "    def useWeights(self,embed_dim,X):\n",
    "        embeddings_index = {}\n",
    "        VOCAB=X['statements'][1].keys()\n",
    "        vocabDic=VOCAB=X['statements'][1]\n",
    "        EMBEDDING_DIM=embed_dim\n",
    "        embedding_matrix = np.zeros((len(VOCAB) + 1, EMBEDDING_DIM))\n",
    "        i=0\n",
    "        with open('wiki-news-300d-1M.vec') as f:\n",
    "            next(f)\n",
    "            for line in f:\n",
    "                values = line.split()\n",
    "                word = values[0]\n",
    "\n",
    "                if word in VOCAB:\n",
    "                    embedding_matrix[vocabDic[word],:]=np.asarray(values[1:EMBEDDING_DIM+1], dtype='float32')\n",
    "                    i=i+1\n",
    "        return (embedding_matrix)\n",
    "    \n",
    "    def fit(self, X, y,epochs=8):\n",
    "        #pdb.set_trace()\n",
    "        \n",
    "        mainIn=Input(shape=(self.MAX_SEQUENCE_LENGTH,), dtype='int32', name='main_input')\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "        \n",
    "        x=Embedding(len(vocabDic)+1,self.EMBED_DIM,input_length=self.MAX_SEQUENCE_LENGTH)(mainIn)\n",
    "        \n",
    "        x=Reshape((1,self.MAX_SEQUENCE_LENGTH, self.EMBED_DIM))(x)\n",
    "        \n",
    "        x1=Conv2D(128, strides=2,kernel_size=5 ,activation=\"relu\", padding='same')(x)\n",
    "\n",
    "        x1=MaxPooling2D((self.MAX_SEQUENCE_LENGTH-5+1,1),padding='same')(x1)\n",
    "        x1=Flatten()(x1)\n",
    "        x2=Conv2D(128, strides=2, kernel_size=4, activation=\"relu\", padding='same')(x)\n",
    "        x2=MaxPooling2D((self.MAX_SEQUENCE_LENGTH-4+1,1),padding='same')(x2)\n",
    "        \n",
    "        x2=Flatten()(x2)\n",
    "        x3=Conv2D(128, strides=2, kernel_size=3, activation=\"relu\", padding='same')(x)\n",
    "        x3=MaxPooling2D((self.MAX_SEQUENCE_LENGTH-3+1,1),padding='same')(x3)\n",
    "        \n",
    "        x3=Flatten()(x3)\n",
    "        \n",
    "        combinedX=keras.layers.concatenate([x1,x2,x3],axis=1)\n",
    "        \n",
    "        \n",
    "        combinedX=Dense(128, activation=\"relu\")(combinedX)\n",
    "        combinedX=Dropout(0.3)(combinedX)\n",
    "        output=Dense(self.nb_labels, activation=\"softmax\")(combinedX)\n",
    "\n",
    "        \n",
    "        encoder =preprocessing.LabelEncoder()\n",
    "        encoder.fit(y)\n",
    "        encoded_Y = encoder.transform(y)\n",
    "        labels=keras.utils.to_categorical(encoded_Y, num_classes=self.nb_labels)\n",
    "        \n",
    "        #labels=y\n",
    "        inputs=X\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.model = Model(inputs=[mainIn], outputs=[output])\n",
    "\n",
    "        self.model.compile(loss='categorical_crossentropy',\n",
    "                        optimizer='rmsprop',\n",
    "                        metrics=['acc'])\n",
    "        \n",
    "        self.model.fit(inputs,labels,epochs=epochs, batch_size=self.batch_size)\n",
    "    def predict(self,data):\n",
    "        return(np.argmax(self.model.predict(data),axis=1))\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model=Classifier(vocabDic,nb_labels=2)\n",
    "model.fit(train_set[:500000],train_label[:500000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.predict(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_label[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model=Classifier(vocabDic)\n",
    "model.fit(train_set[:500000],train_label[:500000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.model.save(\"modelCNN.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "modelTest= load_model(\"modelCNN.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "modelTest.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions=model.model.predict(test_set)\n",
    "predicted_labels=np.argmax(predictions,axis=1)\n",
    "correct_predictions=sum(predicted_labels==test_label)\n",
    "print(correct_predictions/len(test_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a simple LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv2D\n",
    "from keras.models import Sequential\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Reshape\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Input\n",
    "from keras import backend as K\n",
    "from keras.layers import LSTM\n",
    "from keras.models import Model\n",
    "import tensorflow as tf\n",
    "from sklearn import preprocessing\n",
    "import keras\n",
    "import numpy as np\n",
    "import pdb\n",
    "\n",
    "#using multiple filters\n",
    "config = tf.ConfigProto()\n",
    " \n",
    "# Don't pre-allocate memory; allocate as-needed\n",
    "config.gpu_options.allow_growth = True\n",
    " \n",
    "# Only allow a total of half the GPU memory to be allocated\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
    " \n",
    "# Create a session with the above options specified.\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
    "K.tensorflow_backend.set_session(tf.Session(config=config))\n",
    "class ClassifierLSTM():\n",
    "    def __init__(self,Vocab,maxlen,nb_labels=2):\n",
    "        self.model=Sequential()\n",
    "        self.EMBED_DIM=30\n",
    "        self.batch_size=30\n",
    "        self.MAX_SEQUENCE_LENGTH=maxlen\n",
    "        self.nb_labels=nb_labels\n",
    "        self.model = Sequential()\n",
    "        self.Vocab=Vocab\n",
    "    def useWeights(self,embed_dim,X):\n",
    "        embeddings_index = {}\n",
    "        VOCAB=X['statements'][1].keys()\n",
    "        vocabDic=VOCAB=X['statements'][1]\n",
    "        EMBEDDING_DIM=embed_dim\n",
    "        embedding_matrix = np.zeros((len(VOCAB) + 1, EMBEDDING_DIM))\n",
    "        i=0\n",
    "        with open('wiki-news-300d-1M.vec') as f:\n",
    "            next(f)\n",
    "            for line in f:\n",
    "                values = line.split()\n",
    "                word = values[0]\n",
    "\n",
    "                if word in VOCAB:\n",
    "                    embedding_matrix[vocabDic[word],:]=np.asarray(values[1:EMBEDDING_DIM+1], dtype='float32')\n",
    "                    i=i+1\n",
    "        return (embedding_matrix)\n",
    "    \n",
    "    def fit(self, X,y,epochs=3):\n",
    "        #pdb.set_trace()\n",
    "        \n",
    "        mainIn=Input(shape=(self.MAX_SEQUENCE_LENGTH,), dtype='int32', name='main_input')\n",
    "        \n",
    "        x=Embedding(len(self.Vocab)+2,self.EMBED_DIM,input_length=self.MAX_SEQUENCE_LENGTH)(mainIn)\n",
    "        \n",
    "        #x=Reshape((1,self.MAX_SEQUENCE_LENGTH, self.EMBED_DIM))(x)\n",
    "        \n",
    "        x=LSTM(self.EMBED_DIM, dropout=0.2, recurrent_dropout=0.2)(x)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        output=Dense(self.nb_labels, activation='softmax')(x)\n",
    "        \n",
    "        encoder =preprocessing.LabelEncoder()\n",
    "        encoder.fit(y)\n",
    "        encoded_Y = encoder.transform(y)\n",
    "        labels=keras.utils.to_categorical(encoded_Y, num_classes=self.nb_labels)\n",
    "        #labels=y\n",
    "        inputs=X\n",
    "        self.model = Model(inputs=mainIn, outputs=output)\n",
    "        \n",
    "        self.model.compile(loss='categorical_crossentropy',\n",
    "                        optimizer='adam',\n",
    "                        metrics=['acc'])\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        self.model.summary\n",
    "        self.model.fit(inputs,labels,epochs=epochs, batch_size=self.batch_size)\n",
    "    def predict(self, X):\n",
    "        return np.argmax(self.model.predict(np.array(X)),axis=1)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \n",
    "        return self.model.predict(np.array(X), self.batch_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model=ClassifierLSTM(vocabDic,maxlen,nb_labels=3)\n",
    "model.fit(train_set[:500000],train_label[:500000])\n",
    "predictions=model.predict(test_set)\n",
    "predicted_labels=np.argmax(predictions,axis=1)\n",
    "correct_predictions=sum(predicted_labels==test_label)\n",
    "print(correct_predictions/len(test_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions_bool=predictions>0.5\n",
    "predictions_bool=[1 if x ==True else 0 for x in predictions_bool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "predicted_labels=predictions>0.5\n",
    "correct_predictions=sum(predictions_bool==test_label)\n",
    "print(correct_predictions/len(test_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Using a bidirectional LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv2D\n",
    "from keras.models import Sequential\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Reshape\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Input\n",
    "from keras.layers import Bidirectional\n",
    "from keras import backend as K\n",
    "from keras.layers import LSTM\n",
    "from keras.models import Model\n",
    "import tensorflow as tf\n",
    "from sklearn import preprocessing\n",
    "import keras\n",
    "import numpy as np\n",
    "import pdb\n",
    "\n",
    "#using multiple filters\n",
    "config = tf.ConfigProto()\n",
    " \n",
    "# Don't pre-allocate memory; allocate as-needed\n",
    "config.gpu_options.allow_growth = True\n",
    " \n",
    "# Only allow a total of half the GPU memory to be allocated\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
    " \n",
    "# Create a session with the above options specified.\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
    "K.tensorflow_backend.set_session(tf.Session(config=config))\n",
    "class ClassifierBidirectionalLSTM():\n",
    "    def __init__(self,Vocab,maxlen,nb_labels=2):\n",
    "        self.model=Sequential()\n",
    "        self.EMBED_DIM=30\n",
    "        self.batch_size=30\n",
    "        self.MAX_SEQUENCE_LENGTH=maxlen\n",
    "        self.nb_labels=nb_labels\n",
    "        self.model = Sequential()\n",
    "        self.Vocab=Vocab\n",
    "    def useWeights(self,embed_dim,X):\n",
    "        embeddings_index = {}\n",
    "        VOCAB=X['statements'][1].keys()\n",
    "        vocabDic=VOCAB=X['statements'][1]\n",
    "        EMBEDDING_DIM=embed_dim\n",
    "        embedding_matrix = np.zeros((len(VOCAB) + 1, EMBEDDING_DIM))\n",
    "        i=0\n",
    "        with open('wiki-news-300d-1M.vec') as f:\n",
    "            next(f)\n",
    "            for line in f:\n",
    "                values = line.split()\n",
    "                word = values[0]\n",
    "\n",
    "                if word in VOCAB:\n",
    "                    embedding_matrix[vocabDic[word],:]=np.asarray(values[1:EMBEDDING_DIM+1], dtype='float32')\n",
    "                    i=i+1\n",
    "        return (embedding_matrix)\n",
    "    \n",
    "    def fit(self, X,y,epochs):\n",
    "        #pdb.set_trace()\n",
    "        \n",
    "        mainIn=Input(shape=(self.MAX_SEQUENCE_LENGTH,), dtype='int32', name='main_input')\n",
    "        \n",
    "        x=Embedding(len(self.Vocab)+2,self.EMBED_DIM,input_length=self.MAX_SEQUENCE_LENGTH)(mainIn)\n",
    "        \n",
    "        #x=Reshape((1,self.MAX_SEQUENCE_LENGTH, self.EMBED_DIM))(x)\n",
    "        \n",
    "        x=Bidirectional(LSTM(self.EMBED_DIM, dropout=0.2, recurrent_dropout=0.2))(x)\n",
    "        output=Dense(self.nb_labels, activation='softmax')(x)\n",
    "        \n",
    "        encoder =preprocessing.LabelEncoder()\n",
    "        encoder.fit(y)\n",
    "        encoded_Y = encoder.transform(y)\n",
    "        labels=keras.utils.to_categorical(encoded_Y, num_classes=self.nb_labels)\n",
    "        #labels=y\n",
    "        inputs=X\n",
    "        self.model = Model(inputs=mainIn, outputs=output)\n",
    "        \n",
    "        self.model.compile(loss='categorical_crossentropy',\n",
    "                        optimizer='adam',\n",
    "                        metrics=['acc'])\n",
    "        self.model.summary\n",
    "        self.model.fit(inputs,labels,epochs=epochs, batch_size=self.batch_size)\n",
    "    def predict(self, X):\n",
    "        return np.argmax(self.model.predict(np.array(X)),axis=1)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \n",
    "        return self.model.predict(np.array(X), self.batch_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model=ClassifierBidirectionalLSTM(vocabDic,maxlen,nb_labels=3)\n",
    "model.fit(train_set[:500000],train_label[:500000])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions=model.predict(test_set)\n",
    "predictions_bool=predictions>0.5\n",
    "predictions_bool=[1 if x ==True else 0 for x in predictions_bool]\n",
    "correct_predictions=sum(predictions_bool==test_label)\n",
    "print(correct_predictions/len(test_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Using a vanilla RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv2D\n",
    "from keras.models import Sequential\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Reshape\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Input\n",
    "from keras.layers import Bidirectional\n",
    "from keras import backend as K\n",
    "from keras.layers import LSTM\n",
    "\n",
    "\n",
    "from keras.layers import SimpleRNN\n",
    "from keras.models import Model\n",
    "import tensorflow as tf\n",
    "from sklearn import preprocessing\n",
    "import keras\n",
    "import numpy as np\n",
    "import pdb\n",
    "\n",
    "#using multiple filters\n",
    "config = tf.ConfigProto()\n",
    " \n",
    "# Don't pre-allocate memory; allocate as-needed\n",
    "config.gpu_options.allow_growth = True\n",
    " \n",
    "# Only allow a total of half the GPU memory to be allocated\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
    " \n",
    "# Create a session with the above options specified.\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
    "K.tensorflow_backend.set_session(tf.Session(config=config))\n",
    "class ClassifierRNN():\n",
    "    def __init__(self,Vocab,maxlen,nb_labels=2):\n",
    "        self.model=Sequential()\n",
    "        self.EMBED_DIM=30\n",
    "        self.batch_size=30\n",
    "        self.MAX_SEQUENCE_LENGTH=maxlen\n",
    "        self.nb_labels=nb_labels\n",
    "        self.model = Sequential()\n",
    "        self.Vocab=Vocab\n",
    "    def useWeights(self,embed_dim,X):\n",
    "        embeddings_index = {}\n",
    "        VOCAB=X['statements'][1].keys()\n",
    "        vocabDic=VOCAB=X['statements'][1]\n",
    "        EMBEDDING_DIM=embed_dim\n",
    "        embedding_matrix = np.zeros((len(VOCAB) + 1, EMBEDDING_DIM))\n",
    "        i=0\n",
    "        with open('wiki-news-300d-1M.vec') as f:\n",
    "            next(f)\n",
    "            for line in f:\n",
    "                values = line.split()\n",
    "                word = values[0]\n",
    "\n",
    "                if word in VOCAB:\n",
    "                    embedding_matrix[vocabDic[word],:]=np.asarray(values[1:EMBEDDING_DIM+1], dtype='float32')\n",
    "                    i=i+1\n",
    "        return (embedding_matrix)\n",
    "    \n",
    "    def fit(self, X,y,epochs=3):\n",
    "        #pdb.set_trace()\n",
    "        \n",
    "        mainIn=Input(shape=(self.MAX_SEQUENCE_LENGTH,), dtype='int32', name='main_input')\n",
    "        \n",
    "        x=Embedding(len(self.Vocab)+2,self.EMBED_DIM,input_length=self.MAX_SEQUENCE_LENGTH)(mainIn)\n",
    "\n",
    "        #x=Reshape((1,self.MAX_SEQUENCE_LENGTH, self.EMBED_DIM))(x)\n",
    "        x=SimpleRNN(self.EMBED_DIM, activation='tanh', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros',dropout=0.0, recurrent_dropout=0.0)(x)\n",
    "\n",
    "        output=Dense(1, activation='sigmoid')(x)\n",
    "        \n",
    "        #encoder =preprocessing.LabelEncoder()\n",
    "        #encoder.fit(y)\n",
    "        #encoded_Y = encoder.transform(y)\n",
    "        #labels=keras.utils.to_categorical(encoded_Y, num_classes=2)\n",
    "        \n",
    "        output=Dense(self.nb_labels, activation='softmax')(x)\n",
    "        \n",
    "        encoder =preprocessing.LabelEncoder()\n",
    "        encoder.fit(y)\n",
    "        encoded_Y = encoder.transform(y)\n",
    "        labels=keras.utils.to_categorical(encoded_Y, num_classes=self.nb_labels)\n",
    "        #labels=y\n",
    "        inputs=X\n",
    "        self.model = Model(inputs=mainIn, outputs=output)\n",
    "        \n",
    "        self.model.compile(loss='categorical_crossentropy',\n",
    "                        optimizer='adam',\n",
    "                        metrics=['acc'])\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.model.summary\n",
    "        self.model.fit(inputs,labels,epochs=epochs, batch_size=self.batch_size)\n",
    "    def predict(self, X):\n",
    "        return np.argmax(self.model.predict(np.array(X)),axis=1)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \n",
    "        return self.model.predict(np.array(X), self.batch_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      " 90480/500000 [====>.........................] - ETA: 823s - loss: 0.5253 - acc: 0.7364"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-6ba82829b4af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mClassifierRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabDic\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmaxlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m500000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_label\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m500000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-0ff5108f1483>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1596\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1597\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1598\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1599\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1600\u001b[0m     def evaluate(self, x, y,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1181\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1183\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1184\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2271\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2272\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2273\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2274\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model=ClassifierRNN(vocabDic,maxlen)\n",
    "model.fit(train_set[:500000],train_label[:500000])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions=model.model.predict(test_set)\n",
    "predictions_bool=predictions>0.5\n",
    "predictions_bool=[1 if x ==True else 0 for x in predictions_bool]\n",
    "correct_predictions=sum(predictions_bool==test_label)\n",
    "print(correct_predictions/len(test_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# The second dataset model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipping line 1761: expected 4 fields, saw 5\n",
      "Skipping line 3104: expected 4 fields, saw 5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "train=pd.read_csv(\"train.csv\",error_bad_lines=False,delimiter='\\t')\n",
    "test=pd.read_csv(\"test.csv\",error_bad_lines=False,delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modifyDocument(file):\n",
    "    return(file['Sentiment'].apply(lambda x: 'neutral' if 'objective' in x else x ))\n",
    "train_labels=modifyDocument(train)\n",
    "test_labels=modifyDocument(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=train['Tweet']\n",
    "test_data=test['Tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabDic=dict()\n",
    "for document in train_data:\n",
    "    document=document.split(\" \")\n",
    "    for word in document:\n",
    "        if word not in vocabDic:\n",
    "            vocabDic[word]=len(vocabDic)+1\n",
    "vocabDic['unk']=len(vocabDic)+1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def codeDocuments(documents,dictionnary):\n",
    "    documentsArray=list()\n",
    "    for i,document in enumerate(documents):\n",
    "        tempList=list()\n",
    "        document=document.split(\" \")\n",
    "        for word in document:\n",
    "            if word in vocabDic:\n",
    "                word=vocabDic[word]\n",
    "            else:\n",
    "                word=vocabDic['unk']\n",
    "            tempList.append(word)\n",
    "        documentsArray.append(tempList)\n",
    "    return np.array(documentsArray)\n",
    "train_docs=codeDocuments(train_data,vocabDic)\n",
    "test_docs=codeDocuments(test_data,vocabDic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]),\n",
       "       list([15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36]),\n",
       "       list([23, 37, 38, 39, 40, 41, 42, 43, 44, 45, 31, 46, 47, 48, 49, 50, 33, 9, 49, 51, 52, 53, 54, 55]),\n",
       "       ...,\n",
       "       list([1324, 63, 755, 126, 91, 2581, 121, 29, 28305, 191, 129, 28306, 915, 482, 127, 49, 5663, 121, 49, 28307, 27905, 63, 479, 13324, 1331]),\n",
       "       list([1324, 8799, 30, 496, 22992, 31, 3722, 597, 129, 4987, 28308, 121, 6452, 494, 129, 28309]),\n",
       "       list([28310, 2420, 49, 387, 171, 28119, 15944, 3820, 1026])], dtype=object)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "maxlen=35\n",
    "train_set = sequence.pad_sequences(train_docs, maxlen=maxlen)\n",
    "test_set = sequence.pad_sequences(test_docs, maxlen=maxlen)\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "#test_labels_encoded=mlb.fit_transform(train_label)\n",
    "#train_labels_encoded=mlb.fit_transform(test_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       positive\n",
      "1       negative\n",
      "2       positive\n",
      "3       negative\n",
      "4       negative\n",
      "5        neutral\n",
      "6        neutral\n",
      "7        neutral\n",
      "8        neutral\n",
      "9        neutral\n",
      "10      positive\n",
      "11       neutral\n",
      "12      positive\n",
      "13       neutral\n",
      "14       neutral\n",
      "15       neutral\n",
      "16       neutral\n",
      "17      positive\n",
      "18      positive\n",
      "19      negative\n",
      "20       neutral\n",
      "21       neutral\n",
      "22       neutral\n",
      "23       neutral\n",
      "24      negative\n",
      "25      positive\n",
      "26      positive\n",
      "27       neutral\n",
      "28       neutral\n",
      "29      positive\n",
      "          ...   \n",
      "5880    positive\n",
      "5881    positive\n",
      "5882    positive\n",
      "5883     neutral\n",
      "5884    positive\n",
      "5885    positive\n",
      "5886     neutral\n",
      "5887    positive\n",
      "5888    positive\n",
      "5889    positive\n",
      "5890    negative\n",
      "5891    negative\n",
      "5892     neutral\n",
      "5893    positive\n",
      "5894    positive\n",
      "5895    positive\n",
      "5896     neutral\n",
      "5897     neutral\n",
      "5898    negative\n",
      "5899    positive\n",
      "5900    positive\n",
      "5901    negative\n",
      "5902    positive\n",
      "5903    positive\n",
      "5904    positive\n",
      "5905    negative\n",
      "5906    positive\n",
      "5907    positive\n",
      "5908     neutral\n",
      "5909    positive\n",
      "Name: Sentiment, Length: 5910, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() takes exactly 1 argument (4 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-171-9adb5fbcf38e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mClassifier22\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabDic\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmaxlen\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnb_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() takes exactly 1 argument (4 given)"
     ]
    }
   ],
   "source": [
    "model=Classifier22(vocabDic,maxlen,nb_labels=3)\n",
    "model.fit(train_set,train_labels,epochs=15)\n",
    "predictions=model.predict(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels=test_labels.replace(\"positive\",2).replace(\"negative\",0).replace(\"neutral\",1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.419428571429\n"
     ]
    }
   ],
   "source": [
    "predictions=model.model.predict(test_set)\n",
    "predicted_labels=np.argmax(predictions,axis=1)\n",
    "correct_predictions=sum(predicted_labels==test_labels)\n",
    "print(correct_predictions*1.0/len(test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model using word to vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "import multiprocessing\n",
    "import tensorflow as tf\n",
    "\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Flatten\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# Set random seed (for reproducibility)\n",
    "np.random.seed(1000)\n",
    "\n",
    "# Select whether using Keras with or without GPU support\n",
    "# See: https://stackoverflow.com/questions/40690598/can-keras-with-tensorflow-backend-be-forced-to-use-cpu-or-gpu-at-will\n",
    "use_gpu = False\n",
    "\n",
    "config = tf.ConfigProto(intra_op_parallelism_threads=multiprocessing.cpu_count(), \n",
    "                        inter_op_parallelism_threads=multiprocessing.cpu_count(), \n",
    "                        allow_soft_placement=True, \n",
    "                        device_count = {'CPU' : 1, \n",
    "                                        'GPU' : 1 if use_gpu else 0})\n",
    "\n",
    "session = tf.Session(config=config)\n",
    "K.set_session(session)\n",
    "\n",
    "dataset_location = 'Sentiment Analysis Dataset.csv'\n",
    "model_location = '/twitter/model/'\n",
    "\n",
    "corpus = []\n",
    "labels = []\n",
    "\n",
    "# Parse tweets and sentiments\n",
    "with open(dataset_location, 'r') as df:\n",
    "    for i, line in enumerate(df):\n",
    "        if i == 0:\n",
    "            # Skip the header\n",
    "            continue\n",
    "\n",
    "        parts = line.strip().split(',')\n",
    "        \n",
    "        # Sentiment (0 = Negative, 1 = Positive)\n",
    "        labels.append(int(parts[1].strip()))\n",
    "        \n",
    "        # Tweet\n",
    "        tweet = parts[3].strip()\n",
    "        if tweet.startswith('\"'):\n",
    "            tweet = tweet[1:]\n",
    "        if tweet.endswith('\"'):\n",
    "            tweet = tweet[::-1]\n",
    "        \n",
    "        corpus.append(tweet.strip().lower())\n",
    "        \n",
    "print('Corpus size: {}'.format(len(corpus)))\n",
    "\n",
    "# Tokenize and stem\n",
    "tkr = RegexpTokenizer('[a-zA-Z0-9@]+')\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "tokenized_corpus = []\n",
    "print \"hello\"\n",
    "for i, tweet in enumerate(corpus):\n",
    "    tokens = [t for t in tkr.tokenize(tweet) if not t.startswith('@')]\n",
    "    tokenized_corpus.append(tokens)\n",
    "print \"hello\"\n",
    "# Gensim Word2Vec model\n",
    "vector_size = 512\n",
    "window_size = 10\n",
    "\n",
    "# Create Word2Vec\n",
    "word2vec = Word2Vec(sentences=tokenized_corpus,\n",
    "                    size=vector_size, \n",
    "                    window=window_size, \n",
    "                    negative=20,\n",
    "                    iter=50,\n",
    "                    seed=1000,\n",
    "                    workers=multiprocessing.cpu_count())\n",
    "\n",
    "# Copy word vectors and delete Word2Vec model  and original corpus to save memory\n",
    "X_vecs = word2vec.wv\n",
    "del word2vec\n",
    "del corpus\n",
    "print \"hello\"\n",
    "# Train subset size (0 < size < len(tokenized_corpus))\n",
    "train_size = 1000000\n",
    "\n",
    "# Test subset size (0 < size < len(tokenized_corpus) - train_size)\n",
    "test_size = 100000\n",
    "\n",
    "# Compute average and max tweet length\n",
    "avg_length = 0.0\n",
    "max_length = 0\n",
    "\n",
    "for tweet in tokenized_corpus:\n",
    "    if len(tweet) > max_length:\n",
    "        max_length = len(tweet)\n",
    "    avg_length += float(len(tweet))\n",
    "    \n",
    "print('Average tweet length: {}'.format(avg_length / float(len(tokenized_corpus))))\n",
    "print('Max tweet length: {}'.format(max_length))\n",
    "\n",
    "# Tweet max length (number of tokens)\n",
    "max_tweet_length = 15\n",
    "\n",
    "# Create train and test sets\n",
    "# Generate random indexes\n",
    "indexes = set(np.random.choice(len(tokenized_corpus), train_size + test_size, replace=False))\n",
    "\n",
    "X_train = np.zeros((train_size, max_tweet_length, vector_size), dtype=K.floatx())\n",
    "Y_train = np.zeros((train_size, 2), dtype=np.int32)\n",
    "X_test = np.zeros((test_size, max_tweet_length, vector_size), dtype=K.floatx())\n",
    "Y_test = np.zeros((test_size, 2), dtype=np.int32)\n",
    "\n",
    "for i, index in enumerate(indexes):\n",
    "    for t, token in enumerate(tokenized_corpus[index]):\n",
    "        if t >= max_tweet_length:\n",
    "            break\n",
    "        \n",
    "        if token not in X_vecs:\n",
    "            continue\n",
    "    \n",
    "        if i < train_size:\n",
    "            X_train[i, t, :] = X_vecs[token]\n",
    "        else:\n",
    "            X_test[i - train_size, t, :] = X_vecs[token]\n",
    "            \n",
    "    if i < train_size:\n",
    "        Y_train[i, :] = [1.0, 0.0] if labels[index] == 0 else [0.0, 1.0]\n",
    "    else:\n",
    "        Y_test[i - train_size, :] = [1.0, 0.0] if labels[index] == 0 else [0.0, 1.0]\n",
    "        \n",
    "# Keras convolutional model\n",
    "batch_size = 32\n",
    "nb_epochs = 100\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv1D(32, kernel_size=3, activation='elu', padding='same', input_shape=(max_tweet_length, vector_size)))\n",
    "model.add(Conv1D(32, kernel_size=3, activation='elu', padding='same'))\n",
    "model.add(Conv1D(32, kernel_size=3, activation='elu', padding='same'))\n",
    "model.add(Conv1D(32, kernel_size=3, activation='elu', padding='same'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv1D(32, kernel_size=2, activation='elu', padding='same'))\n",
    "model.add(Conv1D(32, kernel_size=2, activation='elu', padding='same'))\n",
    "model.add(Conv1D(32, kernel_size=2, activation='elu', padding='same'))\n",
    "model.add(Conv1D(32, kernel_size=2, activation='elu', padding='same'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(256, activation='tanh'))\n",
    "model.add(Dense(256, activation='tanh'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(lr=0.0001, decay=1e-6),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train, Y_train,\n",
    "          batch_size=batch_size,\n",
    "          shuffle=True,\n",
    "          epochs=nb_epochs,\n",
    "          validation_data=(X_test, Y_test),\n",
    "          callbacks=[EarlyStopping(min_delta=0.00025, patience=2)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
